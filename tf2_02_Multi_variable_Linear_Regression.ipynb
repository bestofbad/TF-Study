{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d5bd8f",
   "metadata": {},
   "source": [
    "# 02. Multi-variable Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac397b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a41fd8",
   "metadata": {},
   "source": [
    "## 2-1. Simple Example (2 variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be1d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)        # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804012fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_data = [1, 0, 3, 0, 5]\n",
    "x2_data = [0, 2, 0, 4, 0]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "# tf.random.uniform(shape=[1], minval=-10.0, maxval=10.0)\n",
    "W1 = tf.Variable(tf.random.uniform([1], -10.0, 10.0))   \n",
    "W2 = tf.Variable(tf.random.uniform([1], -10.0, 10.0))\n",
    "b  = tf.Variable(tf.random.uniform([1], -10.0, 10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fafa24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  62.234009 |     0.7571 |    -3.4614 |   1.957940\n",
      "  100 |  12.134546 |     0.4957 |    -1.3739 |   2.437738\n",
      "  200 |   3.236942 |     0.3824 |    -0.4835 |   2.586154\n",
      "  300 |   1.602640 |     0.3436 |    -0.0952 |   2.600542\n",
      "  400 |   1.253230 |     0.3385 |     0.0820 |   2.560854\n",
      "  500 |   1.134123 |     0.3478 |     0.1702 |   2.499999\n",
      "  600 |   1.059052 |     0.3629 |     0.2205 |   2.431620\n",
      "  700 |   0.994950 |     0.3801 |     0.2547 |   2.361372\n",
      "  800 |   0.935795 |     0.3980 |     0.2815 |   2.291592\n",
      "  900 |   0.880347 |     0.4157 |     0.3050 |   2.223218\n",
      " 1000 |   0.828216 |     0.4332 |     0.3267 |   2.156609\n"
     ]
    }
   ],
   "source": [
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W1 * x1_data + W2 * x2_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n",
    "    W1.assign_sub(learning_rate * W1_grad)\n",
    "    W2.assign_sub(learning_rate * W2_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302f67e6",
   "metadata": {},
   "source": [
    "## 2-2. Simple Example (2 variables with Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f3fd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data dim.: [2 5]\n",
      "y_data dim.: [5]\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1., 0., 3., 0., 5.],\n",
    "          [0., 2., 0., 4., 0.]]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "print('x_data dim.:', tf.shape(x_data).numpy())\n",
    "print('y_data dim.:', tf.shape(y_data).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88318502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W 차원 (1, 2) : W * (2, 5)  = (1, 5) \n",
    "W = tf.Variable(tf.random.uniform([1, 2], -1.0, 1.0))\n",
    "b = tf.Variable(tf.random.uniform([1], -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8cfb28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.81983495 0.33535123]]\n",
      "[-0.9917586]\n"
     ]
    }
   ],
   "source": [
    "print(W.numpy())\n",
    "print(b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf13a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   5.203093 |     0.8259 |     0.3430 |  -0.987531\n",
      "  100 |   0.717020 |     1.1153 |     0.8421 |  -0.733672\n",
      "  200 |   0.165334 |     1.1583 |     1.0409 |  -0.638880\n",
      "  300 |   0.076656 |     1.1571 |     1.1196 |  -0.593822\n",
      "  400 |   0.058726 |     1.1503 |     1.1492 |  -0.566144\n",
      "  500 |   0.052911 |     1.1442 |     1.1586 |  -0.545225\n",
      "  600 |   0.049367 |     1.1390 |     1.1596 |  -0.527248\n",
      "  700 |   0.046372 |     1.1344 |     1.1573 |  -0.510745\n",
      "  800 |   0.043614 |     1.1302 |     1.1536 |  -0.495121\n",
      "  900 |   0.041029 |     1.1262 |     1.1494 |  -0.480125\n",
      " 1000 |   0.038600 |     1.1224 |     1.1451 |  -0.465647\n"
     ]
    }
   ],
   "source": [
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) + b               # (1, 2) * (2, 5) = (1, 5)\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2711c912",
   "metadata": {},
   "source": [
    "## 2-3. Simple Example (Hypothesis without b)\n",
    "\n",
    "- 별도의 bias 변수를 설정하지 않고, W 가중치 변수에 반영.\n",
    "- tf1.x 버전에서 optimizer로 사용된 tf.train.GradientDescentOptimizer는 tf.keras.optimizers.SGD로 변경됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d24712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   0.793786 |    -0.2423 |     0.9282 |     0.6723\n",
      "  100 |   0.113464 |    -0.1494 |     1.0171 |     0.8834\n",
      "  200 |   0.020052 |    -0.1161 |     1.0288 |     0.9690\n",
      "  300 |   0.004773 |    -0.1021 |     1.0278 |     1.0039\n",
      "  400 |   0.002062 |    -0.0949 |     1.0257 |     1.0179\n",
      "  500 |   0.001515 |    -0.0904 |     1.0242 |     1.0233\n",
      "  600 |   0.001350 |    -0.0870 |     1.0231 |     1.0251\n",
      "  700 |   0.001257 |    -0.0841 |     1.0222 |     1.0254\n",
      "  800 |   0.001180 |    -0.0815 |     1.0215 |     1.0251\n",
      "  900 |   0.001110 |    -0.0790 |     1.0208 |     1.0245\n",
      " 1000 |   0.001044 |    -0.0766 |     1.0201 |     1.0238\n"
     ]
    }
   ],
   "source": [
    "# 앞의 코드에서 bias(b)를 행렬에 추가\n",
    "x_data = [[1., 1., 1., 1., 1.],     # bias를 '1'로 설정했으나, W가 곱해지고 열 원소가 더해지면\n",
    "          [1., 0., 3., 0., 5.],     # 결과적으로 변수 b 원소가 더해진 것과 동일한 상황.\n",
    "          [0., 2., 0., 4., 0.]]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform([1, 3], -1.0, 1.0))  # [1, 3]으로 변경하고 X_data와 연산을 하면, bias가 반영됨.\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)    #tf.1x버전의 tf.train.GradientDescentOptimizer를 대체\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data)                # bias 없음\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "    grads = tape.gradient(cost, [W])\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads,[W]))\n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea29e4",
   "metadata": {},
   "source": [
    "## 2.4 Simple Example (Custom Gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783381ed",
   "metadata": {},
   "source": [
    "- tf1.x 버전에서 optimizer로 사용된 tf.train.GradientDescentOptimizer는 tf.keras.optimizers.SGD로 변경됨\n",
    "- optimizer.apply_gradients(): update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b506ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 |  11.468436\n",
      "  100 |   0.171258\n",
      "  200 |   0.080056\n",
      "  300 |   0.037423\n",
      "  400 |   0.017494\n",
      "  500 |   0.008178\n",
      "  600 |   0.003823\n",
      "  700 |   0.001787\n",
      "  800 |   0.000835\n",
      "  900 |   0.000390\n",
      " 1000 |   0.000183\n"
     ]
    }
   ],
   "source": [
    "# Multi-variable linear regression (1)\n",
    "\n",
    "X = tf.constant([[1., 2.], \n",
    "                 [3., 4.]])\n",
    "y = tf.constant([[1.5], [3.5]])\n",
    "\n",
    "W = tf.Variable(tf.random.normal([2, 1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   #tf.1x버전의 tf.train.GradientDescentOptimizer를 대체\n",
    "\n",
    "n_epoch = 1000+1\n",
    "print(\"epoch | cost\")\n",
    "for i in range(n_epoch):\n",
    "    # Use tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.matmul(X, W) + b\n",
    "        cost = tf.reduce_mean(tf.square(y_pred - y))\n",
    "\n",
    "    # calculates the gradients of the loss\n",
    "    grads = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    # updates parameters (W and b)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.6f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5abed1",
   "metadata": {},
   "source": [
    "## 2-5. More Examples (Predicting exam score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a7e5e",
   "metadata": {},
   "source": [
    "hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e941bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 | 5793889.5000\n",
      "  100 |     715.2903\n",
      "  200 |       2.0152\n",
      "  300 |       1.9210\n",
      "  400 |       1.9145\n",
      "  500 |       1.9081\n",
      "  600 |       1.9018\n",
      "  700 |       1.8955\n",
      "  800 |       1.8892\n",
      "  900 |       1.8829\n",
      " 1000 |       1.8767\n"
     ]
    }
   ],
   "source": [
    "# data and label\n",
    "x1 = [ 73.,  93.,  89.,  96.,  73.]\n",
    "x2 = [ 80.,  88.,  91.,  98.,  66.]\n",
    "x3 = [ 75.,  93.,  90., 100.,  70.]\n",
    "Y  = [152., 185., 180., 196., 142.]\n",
    "\n",
    "# weights\n",
    "w1 = tf.Variable(10.)\n",
    "w2 = tf.Variable(10.)\n",
    "w3 = tf.Variable(10.)\n",
    "b  = tf.Variable(10.)\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "for i in range(1000+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "    # calculates the gradients of the cost\n",
    "    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b])\n",
    "    \n",
    "    # update w1,w2,w3 and b\n",
    "    w1.assign_sub(learning_rate * w1_grad)\n",
    "    w2.assign_sub(learning_rate * w2_grad)\n",
    "    w3.assign_sub(learning_rate * w3_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "      print(\"{:5} | {:12.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9a3d29",
   "metadata": {},
   "source": [
    "- Matrix 및 hypothesis 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2462888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 |  3658.0347\n",
      "  100 |    19.8423\n",
      "  200 |    19.2920\n",
      "  300 |    19.1901\n",
      "  400 |    19.0889\n",
      "  500 |    18.9882\n",
      "  600 |    18.8881\n",
      "  700 |    18.7885\n",
      "  800 |    18.6894\n",
      "  900 |    18.5908\n",
      " 1000 |    18.4929\n",
      " 1100 |    18.3954\n",
      " 1200 |    18.2984\n",
      " 1300 |    18.2020\n",
      " 1400 |    18.1061\n",
      " 1500 |    18.0108\n",
      " 1600 |    17.9158\n",
      " 1700 |    17.8215\n",
      " 1800 |    17.7276\n",
      " 1900 |    17.6343\n",
      " 2000 |    17.5414\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    # X1,   X2,    X3,   y\n",
    "    [ 73.,  80.,  75., 152. ],\n",
    "    [ 93.,  88.,  93., 185. ],\n",
    "    [ 89.,  91.,  90., 180. ],\n",
    "    [ 96.,  98., 100., 196. ],\n",
    "    [ 73.,  66.,  70., 142. ]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "X = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "\n",
    "W = tf.Variable(tf.random.normal([3, 1]))\n",
    "b = tf.Variable(tf.random.normal([1]))\n",
    "\n",
    "learning_rate = 0.000001\n",
    "\n",
    "# hypothesis, prediction function\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs+1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
    "\n",
    "    # calculates the gradients of the loss\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "\n",
    "    # updates parameters (W and b)\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f911b93",
   "metadata": {},
   "source": [
    "- predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebba0930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[152.0, 185.0, 180.0, 196.0, 142.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y # labels, 실제값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40a7bbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[156.95924],\n",
       "       [180.79514],\n",
       "       [182.22328],\n",
       "       [198.26012],\n",
       "       [136.05217]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X).numpy() # prediction, 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba0cd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[188.56612],\n",
       "       [179.72858]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 데이터에 대한 예측\n",
    "\n",
    "predict([[ 89.,  95.,  92.],[ 84.,  92.,  85.]]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5cdf9",
   "metadata": {},
   "source": [
    "- Reference : https://github.com/deeplearningzerotoall/TensorFlow/blob/master/tf_2.x/lab-04-1-Multi-variable-Linear-Regression--Regression-eager.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
